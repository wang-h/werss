"""æ ‡ç­¾æå–æ¨¡å— - æ”¯æŒ TextRankï¼ˆjiebaï¼‰å’Œ AIï¼ˆDeepSeekï¼‰ä¸¤ç§æ–¹å¼"""
import jieba.analyse
from typing import List, Optional
import os
from core.config import cfg
from core.log import logger
from core.print import print_error, print_success
from core.env_loader import load_dev_env_if_needed

# å°è¯•å¯¼å…¥ AI ç›¸å…³æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from openai import AsyncOpenAI
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    logger.warning("openai æ¨¡å—æœªå®‰è£…ï¼ŒAI æå–åŠŸèƒ½ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥ KeyBERT ç›¸å…³æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from keybert import KeyBERT
    KEYBERT_AVAILABLE = True
except ImportError:
    KEYBERT_AVAILABLE = False
    logger.debug("keybert æ¨¡å—æœªå®‰è£…ï¼ŒKeyBERT æå–åŠŸèƒ½ä¸å¯ç”¨")

# å…¨å±€å•ä¾‹å®ä¾‹ï¼Œç”¨äºå¸¸é©»å†…å­˜
_global_extractor = None


class TagExtractor:
    """æ ‡ç­¾æå–å™¨ï¼Œæ”¯æŒ TextRankã€KeyBERT å’Œ AI ä¸‰ç§æ–¹å¼"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ ‡ç­¾æå–å™¨"""
        self.ai_client = None
        self.ai_model = None
        self.keybert_model = None
        self._custom_tags_cache = None  # ç¼“å­˜ç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾
        
        # åœ¨å¼€å‘ç¯å¢ƒä¸­åŠ è½½ ../.env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        load_dev_env_if_needed()
        
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº† AI
        if AI_AVAILABLE:
            api_key = cfg.get("deepseek.api_key") or os.getenv("DEEPSEEK_API_KEY", "")
            base_url = cfg.get("deepseek.base_url") or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
            model = cfg.get("deepseek.model") or os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
            
            if api_key:
                self.ai_client = AsyncOpenAI(
                    api_key=api_key,
                    base_url=base_url,
                )
                self.ai_model = model
                logger.info(f"DeepSeek API å·²é…ç½®ï¼Œæ¨¡å‹: {model}, Base URL: {base_url}")
            else:
                logger.warning("DeepSeek API Key æœªé…ç½®ï¼ŒAI æå–åŠŸèƒ½ä¸å¯ç”¨")
                logger.debug(f"æ£€æŸ¥è·¯å¾„: cfg.get('deepseek.api_key')={cfg.get('deepseek.api_key')}, os.getenv('DEEPSEEK_API_KEY')={os.getenv('DEEPSEEK_API_KEY', '')}")
        else:
            logger.warning("openai æ¨¡å—æœªå®‰è£…ï¼ŒAI æå–åŠŸèƒ½ä¸å¯ç”¨")
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ KeyBERTï¼ˆæ‡’åŠ è½½ï¼Œåªåœ¨éœ€è¦æ—¶åˆå§‹åŒ–ï¼‰
        self.keybert_available = KEYBERT_AVAILABLE
        # é»˜è®¤ä½¿ç”¨ Model2Vec å¤šè¯­è¨€æ¨¡å‹ï¼ˆCPUå‹å¥½ï¼Œä¸­æ–‡æ”¯æŒå¥½ï¼‰
        # å¯é€‰æ¨¡å‹ï¼š
        # - minishlab/potion-multilingual-128Mï¼ˆæ¨èï¼Œå¤šè¯­è¨€ï¼ŒCPUå‹å¥½ï¼Œå‚æ•°é‡128Mï¼Œä¸‹è½½æ–‡ä»¶çº¦512MBï¼Œè¿è¡Œæ—¶å†…å­˜~200MBï¼‰
        # - paraphrase-multilingual-MiniLM-L12-v2ï¼ˆéœ€è¦PyTorchï¼Œå‚æ•°é‡çº¦118Mï¼Œä¸‹è½½æ–‡ä»¶çº¦500MBï¼Œè¿è¡Œæ—¶å†…å­˜~500MBï¼‰
        # æ³¨æ„ï¼šæ¨¡å‹åç§°ä¸­çš„"128M"æŒ‡çš„æ˜¯å‚æ•°é‡ï¼Œå®é™…ä¸‹è½½æ–‡ä»¶å¤§å°ä¼šæ›´å¤§ï¼ˆåŒ…å«æƒé‡ã€tokenizerã€é…ç½®ç­‰ï¼‰
        # ä½¿ç”¨ get æ–¹æ³•ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨ä¼šä½¿ç”¨é»˜è®¤å€¼ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼‰
        self.keybert_model_name = cfg.get("article_tag.keybert.model") or "minishlab/potion-multilingual-128M"
    
    def _get_custom_tags(self) -> List[str]:
        """
        ä»æ•°æ®åº“è·å–ç”¨æˆ·è‡ªå®šä¹‰çš„æ ‡ç­¾ï¼ˆç”¨äºæ ‡ç­¾æå–æ—¶ä¼˜å…ˆè¯†åˆ«ï¼‰
        
        Returns:
            ç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾åç§°åˆ—è¡¨
        """
        if self._custom_tags_cache is not None:
            return self._custom_tags_cache
        
        try:
            from core.db import DB
            from core.models.tags import Tags
            
            session = DB.get_session()
            try:
                # æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„ç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾
                custom_tags = session.query(Tags).filter(
                    Tags.is_custom == True,
                    Tags.status == 1
                ).all()
                
                # æå–æ ‡ç­¾åç§°
                tag_names = [tag.name for tag in custom_tags if tag.name]
                
                # ç¼“å­˜ç»“æœ
                self._custom_tags_cache = tag_names
                
                if tag_names:
                    logger.debug(f"åŠ è½½äº† {len(tag_names)} ä¸ªç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾: {tag_names[:5]}...")
                
                return tag_names
            finally:
                session.close()
        except Exception as e:
            logger.warning(f"è·å–ç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾å¤±è´¥: {e}")
            # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…å½±å“æ­£å¸¸åŠŸèƒ½
            self._custom_tags_cache = []
            return []
    
    def refresh_custom_tags_cache(self):
        """åˆ·æ–°ç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾ç¼“å­˜"""
        self._custom_tags_cache = None
    
    def _html_to_text(self, html_content: str, to_markdown: bool = False) -> str:
        """
        å°† HTML å†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬æˆ– Markdownï¼Œç”¨äºå…³é”®è¯æå–
        
        Args:
            html_content: HTML å†…å®¹
            to_markdown: æ˜¯å¦è½¬æ¢ä¸º Markdownï¼ˆTrueï¼‰è¿˜æ˜¯çº¯æ–‡æœ¬ï¼ˆFalseï¼‰
            
        Returns:
            è½¬æ¢åçš„æ–‡æœ¬
        """
        if not html_content:
            return html_content
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å« HTML æ ‡ç­¾
        if '<' not in html_content or '>' not in html_content:
            return html_content
        
        try:
            from bs4 import BeautifulSoup
            import re
            
            # è§£æ HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤ script å’Œ style æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆè¿™äº›å¯èƒ½åŒ…å« CSS æ ·å¼å’Œ JavaScriptï¼‰
            for script in soup(["script", "style"]):
                script.decompose()
            
            # ç§»é™¤æ‰€æœ‰å…ƒç´ çš„å†…è”æ ·å¼å±æ€§å’Œ class å±æ€§ï¼Œé¿å…æå–åˆ° CSS æ ·å¼ä¿¡æ¯
            for tag in soup.find_all(True):
                if 'style' in tag.attrs:
                    del tag.attrs['style']
                if 'class' in tag.attrs:
                    del tag.attrs['class']
            
            if to_markdown:
                # è½¬æ¢ä¸º Markdown
                try:
                    from markdownify import markdownify as md
                    # å…ˆæ¸…ç† HTMLï¼Œç§»é™¤ä¸å¿…è¦çš„æ ‡ç­¾
                    for tag in soup.find_all(['span', 'font']):
                        tag.unwrap()
                    # è½¬æ¢ HTML åˆ° Markdown
                    text = md(str(soup), heading_style="ATX", bullets='-*+')
                    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
                    text = re.sub(r'[ \t]+', ' ', text)
                    return text.strip()
                except ImportError:
                    logger.warning("markdownify æœªå®‰è£…ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æå–")
                    to_markdown = False
            
            if not to_markdown:
                # è½¬æ¢ä¸ºçº¯æ–‡æœ¬
                text = soup.get_text(separator=' ', strip=True)
                # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                text = re.sub(r'\s+', ' ', text)
                # è¿‡æ»¤æ‰å¸¸è§çš„å­—ä½“åç§°ï¼ˆé¿å…è¢«æå–ä¸ºæ ‡ç­¾ï¼‰
                font_names = ['Helvetica', 'Arial', 'Times New Roman', 'Courier New', 
                             'Verdana', 'Georgia', 'Palatino', 'Garamond', 'Bookman',
                             'Comic Sans MS', 'Trebuchet MS', 'Impact', 'Lucida Console',
                             'Tahoma', 'Courier', 'Monaco', 'Menlo', 'Consolas',
                             'Roboto', 'Open Sans', 'Lato', 'Montserrat', 'Source Sans Pro']
                for font in font_names:
                    # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯åˆ åŒ…å«è¿™äº›è¯çš„æ­£å¸¸æ–‡æœ¬
                    text = re.sub(r'\b' + re.escape(font) + r'\b', '', text, flags=re.IGNORECASE)
                # å†æ¬¡æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                text = re.sub(r'\s+', ' ', text).strip()
                return text
            
        except Exception as e:
            logger.warning(f"HTML è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
            return html_content
    
    def _extract_phrases(self, text: str) -> List[str]:
        """
        æå–çŸ­è¯­ï¼ˆN-gramï¼‰ï¼šåè¯+åè¯ã€å½¢å®¹è¯+åè¯ç­‰ç»„åˆ
        
        Args:
            text: è¦æå–çŸ­è¯­çš„æ–‡æœ¬
            
        Returns:
            çŸ­è¯­åˆ—è¡¨
        """
        import jieba.posseg as pseg
        
        phrases = []
        words = list(pseg.cut(text))
        
        # æå–2-3ä¸ªè¯çš„ç»„åˆçŸ­è¯­
        for i in range(len(words)):
            # 2-gramï¼šåè¯+åè¯ã€å½¢å®¹è¯+åè¯
            if i < len(words) - 1:
                w1, pos1 = words[i]
                w2, pos2 = words[i+1]
                
                # è·³è¿‡å•å­—è¯å¼€å¤´çš„çŸ­è¯­ï¼ˆé¿å…"è¡Œä»£ç "è¿™ç§ä¸å®Œæ•´ç‰‡æ®µï¼‰
                if len(w1) == 1:
                    continue
                
                # n+n, a+n, nz+n, n+nz, nz+nz
                if (pos1 in ['n', 'nz', 'a', 'nt', 'nr'] and pos2 in ['n', 'nz', 'nt', 'nr']):
                    phrase = w1 + w2
                    # ç¡®ä¿çŸ­è¯­é•¿åº¦åˆç†ï¼ˆè‡³å°‘2ä¸ªå­—ç¬¦ï¼Œé¿å…å•å­—+å•å­—ï¼‰
                    if len(phrase) >= 2 and len(phrase) <= 10:
                        phrases.append(phrase)
            
            # 3-gramï¼šåè¯+åè¯+åè¯ã€å½¢å®¹è¯+åè¯+åè¯
            if i < len(words) - 2:
                w1, pos1 = words[i]
                w2, pos2 = words[i+1]
                w3, pos3 = words[i+2]
                
                # è·³è¿‡å•å­—è¯å¼€å¤´çš„çŸ­è¯­
                if len(w1) == 1:
                    continue
                
                if (pos1 in ['n', 'nz', 'a', 'nt', 'nr'] and 
                    pos2 in ['n', 'nz', 'nt', 'nr'] and 
                    pos3 in ['n', 'nz', 'nt', 'nr']):
                    phrase = w1 + w2 + w3
                    if len(phrase) >= 3 and len(phrase) <= 15:
                        phrases.append(phrase)
        
        return list(dict.fromkeys(phrases))  # å»é‡
    
    def extract_with_textrank(
        self, 
        text: str, 
        topK: int = 5, 
        allowPOS: tuple = ('n', 'nz')
    ) -> List[str]:
        """
        ä½¿ç”¨ jieba TextRank æå–å…³é”®è¯ï¼ˆæ”¹è¿›ç‰ˆï¼šä¼˜å…ˆæå–çŸ­è¯­å’Œä¸“æœ‰åè¯ï¼‰
        
        Args:
            text: è¦æå–å…³é”®è¯çš„æ–‡æœ¬
            topK: è¿”å›å…³é”®è¯æ•°é‡
            allowPOS: å…è®¸çš„è¯æ€§ï¼Œé»˜è®¤ï¼šnï¼ˆåè¯ï¼‰ã€nzï¼ˆå…¶ä»–ä¸“åï¼‰
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            import jieba
            import jieba.posseg as pseg
            
            # åœç”¨è¯åˆ—è¡¨ï¼ˆè¿‡æ»¤æ— æ„ä¹‰çš„é€šç”¨è¯ï¼‰
            stopwords = {
                'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
                'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 
                'è‡ªå·±', 'è¿™'
            }
            
            # ========== æ­¥éª¤0ï¼šæ”¹è¿› jieba åˆ†è¯ï¼Œæ·»åŠ å¸¸è§å®Œæ•´è¯åˆ°è¯å…¸ ==========
            # æ·»åŠ å¸¸è§å®Œæ•´è¯åˆ° jieba è¯å…¸ï¼Œç¡®ä¿å®ƒä»¬è¢«æ­£ç¡®è¯†åˆ«
            common_words = [
                'è‹±ä¼Ÿè¾¾', 'OpenAI', 'Meta', 'DeepSeek', 'Claude', 
                'GPT', 'AIèŠ¯ç‰‡', 'å¤§æ¨¡å‹', 'äººå·¥æ™ºèƒ½'
            ]
            for word in common_words:
                if word in text:
                    jieba.add_word(word, freq=10000, tag='nz')  # é«˜é¢‘ï¼Œç¡®ä¿ä¼˜å…ˆè¯†åˆ«
            
            # ========== æå–çŸ­è¯­å¹¶æ·»åŠ åˆ° jieba è¯å…¸ ==========
            phrases = self._extract_phrases(text)
            
            # å°†çŸ­è¯­ä¸´æ—¶æ·»åŠ åˆ° jieba è¯å…¸ï¼Œç¡®ä¿å®ƒä»¬è¢«å½“ä½œä¸€ä¸ªæ•´ä½“
            for phrase in phrases:
                jieba.add_word(phrase, freq=1000, tag='nz')  # é«˜é¢‘ï¼Œä¸“æœ‰åè¯
            
            # ========== ç¬¬ä¸€æ­¥ï¼šä¼˜å…ˆæå–çŸ­è¯­å’Œä¸“æœ‰åè¯ï¼ˆå…¬å¸åã€äººåï¼‰ ==========
            # å…ˆæ”¶é›†æå–çš„çŸ­è¯­
            entities = phrases.copy()  # çŸ­è¯­ä¼˜å…ˆ
            
            # ç„¶åæå–ä¸“æœ‰åè¯
            # ä½¿ç”¨è¯æ€§æ ‡æ³¨æå–ä¸“æœ‰åè¯ï¼ˆè·³è¿‡å·²ç»åœ¨çŸ­è¯­ä¸­çš„è¯ï¼‰
            words = pseg.cut(text)
            for word, flag in words:
                # nr: äººå, nt: æœºæ„åï¼ˆå…¬å¸åï¼‰, nz: å…¶ä»–ä¸“å
                if flag in ['nr', 'nt', 'nz']:
                    if len(word) >= 2 and len(word) <= 10:
                        if word not in stopwords and not word.isdigit():
                            entities.append(word)
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯èƒ½çš„å…¬å¸åå’Œäººåï¼ˆè‹±æ–‡å’Œä¸­æ–‡ï¼‰
            import re
            
            # æå–è‹±æ–‡ä¸“æœ‰åè¯ï¼ˆé¦–å­—æ¯å¤§å†™çš„å•è¯ï¼ŒåŒ…æ‹¬å•ä¸ªå¤§å†™å­—æ¯+å°å†™å­—æ¯çš„ç»„åˆï¼‰
            # åŒ¹é…å¦‚ï¼šOpenAI, Meta, LeCun, GPT5.2, H200, Skywork, R1V4-Lite, Gemini
            # æ”¹è¿›ï¼šåŒ¹é… "æ˜¯OpenAI"ã€"Metaèƒ½" ç­‰æƒ…å†µ
            english_entities = re.findall(r'(?:æ˜¯|èƒ½|å°†|å·²|æ­£|åœ¨|å’Œ|ä¸|åŠ|æˆ–|çš„|ï¼Œ|ã€|ï¼š|ã€‚)?([A-Z][a-zA-Z0-9]+(?:\-[A-Z][a-zA-Z0-9]+)?)', text)
            for entity in english_entities:
                entity = entity.strip()
                # è¿‡æ»¤æ‰å¤ªçŸ­çš„ï¼ˆå°‘äº2ä¸ªå­—ç¬¦ï¼‰å’Œå¤ªé•¿çš„ï¼ˆè¶…è¿‡20ä¸ªå­—ç¬¦ï¼‰
                if 2 <= len(entity) <= 20:
                    # è¿‡æ»¤æ‰çº¯æ•°å­—
                    if not entity.isdigit():
                        # è¿‡æ»¤æ‰å¸¸è§çš„æ— æ„ä¹‰è¯
                        if entity.lower() not in ['ai', 'pr', 'it', 'id', 'url', 'api']:
                            entities.append(entity)
            
            # æå–å¸¸è§çš„ä¸­æ–‡å…¬å¸åï¼ˆ2-6ä¸ªæ±‰å­—ï¼Œåé¢å¯èƒ½è·Ÿå…¬å¸ã€ç§‘æŠ€ç­‰åç¼€ï¼‰
            # åŒ¹é…å¦‚ï¼šæ˜†ä»‘ä¸‡ç»´ã€è‹±ä¼Ÿè¾¾ã€é˜¿é‡Œã€è…¾è®¯ç­‰
            # æ³¨æ„ï¼šé¿å…æå–"å…¬å¸"ã€"ç§‘æŠ€"ç­‰åç¼€è¯æœ¬èº«
            chinese_company_patterns = [
                r'([\u4e00-\u9fa5]{2,6})(?:å…¬å¸|ç§‘æŠ€|é›†å›¢|è‚¡ä»½|æœ‰é™|æŠ€æœ¯|ä¿¡æ¯|ç½‘ç»œ|è½¯ä»¶|æ•°æ®|æ™ºèƒ½|äººå·¥æ™ºèƒ½|AI|å‘å¸ƒ|å®£å¸ƒ)',
                r'(?:å€ŸåŠ›|åˆä½œ|è”æ‰‹|å’Œ|ä¸|åŠ)([\u4e00-\u9fa5]{2,6})',  # "å€ŸåŠ›é˜¿é‡Œ" -> "é˜¿é‡Œ"
                r'([\u4e00-\u9fa5]{2,6})(?:èƒ½|å°†|å·²|æ­£|åœ¨|è·å‡†|å‡ºå£)',  # "è‹±ä¼Ÿè¾¾H200è·å‡†" -> "è‹±ä¼Ÿè¾¾"
            ]
            company_suffixes = {'å…¬å¸', 'ç§‘æŠ€', 'é›†å›¢', 'è‚¡ä»½', 'æœ‰é™', 'æŠ€æœ¯', 'ä¿¡æ¯', 'ç½‘ç»œ', 'è½¯ä»¶', 'æ•°æ®', 'æ™ºèƒ½'}
            for pattern in chinese_company_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0] if match[0] else (match[1] if len(match) > 1 else '')
                    if isinstance(match, str) and len(match) >= 2 and len(match) <= 10:
                        # è¿‡æ»¤æ‰"å…¬å¸"ã€"ç§‘æŠ€"ç­‰åç¼€è¯æœ¬èº«
                        if match not in company_suffixes and match not in stopwords:
                            entities.append(match)
            
            # ç‰¹åˆ«å¤„ç†ï¼šæå– "è‹±ä¼Ÿè¾¾" è¿™ç§å®Œæ•´çš„å…¬å¸åï¼ˆå³ä½¿è¢«åˆ†è¯ï¼‰
            # åŒ¹é…è¿ç»­çš„2-4ä¸ªæ±‰å­—ï¼Œå¯èƒ½æ˜¯å…¬å¸å
            chinese_multi_char = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
            # å¸¸è§ç§‘æŠ€å…¬å¸åï¼ˆå¯ä»¥æ‰©å±•è¿™ä¸ªåˆ—è¡¨ï¼‰
            known_companies = ['è‹±ä¼Ÿè¾¾', 'æ˜†ä»‘ä¸‡ç»´', 'é˜¿é‡Œ', 'è…¾è®¯', 'ç™¾åº¦', 'å­—èŠ‚', 'ç¾å›¢', 'äº¬ä¸œ', 'å°ç±³', 'åä¸º', 'OPPO', 'vivo']
            
            # è·å–ç”¨æˆ·è‡ªå®šä¹‰çš„æ ‡ç­¾ï¼Œå¹¶åˆå¹¶åˆ° known_companies ä¸­ï¼ˆç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾ä¼˜å…ˆï¼‰
            custom_tags = self._get_custom_tags()
            # åˆå¹¶ï¼šç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾åœ¨å‰ï¼Œç³»ç»Ÿé¢„è®¾æ ‡ç­¾åœ¨å
            all_known_companies = list(dict.fromkeys(custom_tags + known_companies))
            
            # ä¼˜å…ˆåŒ¹é…ç”¨æˆ·è‡ªå®šä¹‰æ ‡ç­¾ï¼Œç„¶ååŒ¹é…ç³»ç»Ÿé¢„è®¾æ ‡ç­¾
            for company in all_known_companies:
                if company in text and company not in entities:
                    entities.append(company)
            
            # æå–ä¸­æ–‡äººåï¼ˆ2-4ä¸ªæ±‰å­—ï¼Œå¸¸è§æ¨¡å¼ï¼‰
            # åŒ¹é…å¦‚ï¼šè´ç´¢æ–¯ã€é©¬æ–¯å…‹ã€æå½¦å®ç­‰
            chinese_person_patterns = [
                r'([\u4e00-\u9fa5]{2,4})(?:è¯´|è¡¨ç¤º|è®¤ä¸º|ç§°|æŒ‡å‡º|å¼ºè°ƒ|é€éœ²|èèµ„|å‘å¸ƒ|å®£å¸ƒ)',
                r'([\u4e00-\u9fa5]{2,4})(?:ï¼š|ï¼Œ|ã€)',  # "è´ç´¢æ–¯èèµ„"ã€"é©¬æ–¯å…‹ï¼š"
            ]
            for pattern in chinese_person_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    if len(match) >= 2 and len(match) <= 4:
                        if match not in stopwords:
                            entities.append(match)
            
            # æå–è‹±æ–‡äººåï¼ˆé¦–å­—æ¯å¤§å†™çš„å•è¯ï¼Œåé¢è·Ÿé€—å·ã€å†’å·ç­‰ï¼‰
            # åŒ¹é…å¦‚ï¼šLeCun, OpenAIç­‰ï¼ˆå·²ç»åœ¨ä¸Šé¢æå–äº†ï¼‰
            
            # å»é‡å¹¶è¿‡æ»¤
            entities = list(dict.fromkeys(entities))
            # è¿‡æ»¤æ‰"å…¬å¸"ã€"ç§‘æŠ€"ç­‰åç¼€è¯æœ¬èº«ï¼Œä»¥åŠä»¥è¿™äº›è¯ç»“å°¾çš„è‹±æ–‡+ä¸­æ–‡ç»„åˆ
            company_suffixes = {'å…¬å¸', 'ç§‘æŠ€', 'é›†å›¢', 'è‚¡ä»½', 'æœ‰é™', 'æŠ€æœ¯', 'ä¿¡æ¯', 'ç½‘ç»œ', 'è½¯ä»¶', 'æ•°æ®', 'æ™ºèƒ½'}
            filtered_entities = []
            for entity in entities:
                # è¿‡æ»¤å…¬å¸åç¼€è¯æœ¬èº«
                if entity in company_suffixes:
                    continue
                # è¿‡æ»¤ä»¥å…¬å¸åç¼€è¯ç»“å°¾çš„è‹±æ–‡+ä¸­æ–‡ç»„åˆï¼ˆå¦‚"DeepSeekå…¬å¸"ï¼‰
                if any(entity.endswith(suffix) for suffix in company_suffixes):
                    # å¦‚æœæ˜¯ä»¥è‹±æ–‡å¼€å¤´çš„è¯ï¼Œè¿‡æ»¤æ‰ï¼ˆå¦‚"DeepSeekå…¬å¸"ï¼‰
                    if re.match(r'^[A-Za-z]', entity):
                        continue
                filtered_entities.append(entity)
            entities = filtered_entities
            
            # ========== ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ TF-IDF å’Œ TextRank æå–å…¶ä»–å…³é”®è¯ ==========
            # ä½¿ç”¨ TF-IDF ä½œä¸ºå¤‡é€‰ï¼ˆé€šå¸¸è´¨é‡æ›´å¥½ï¼‰
            try:
                keywords_tfidf = jieba.analyse.tfidf(
                    text,
                    topK=topK * 2,  # å¤šæå–ä¸€äº›ï¼Œç„¶åè¿‡æ»¤
                    allowPOS=allowPOS
                )
            except:
                keywords_tfidf = []
            
            # ä½¿ç”¨ TextRank
            keywords_textrank = jieba.analyse.textrank(
                text,
                topK=topK * 2,  # å¤šæå–ä¸€äº›ï¼Œç„¶åè¿‡æ»¤
                allowPOS=allowPOS
            )
            
            # åˆå¹¶å¹¶å»é‡
            all_keywords = list(dict.fromkeys(keywords_tfidf + keywords_textrank))
            
            # è¿‡æ»¤æ¡ä»¶
            filtered_keywords = []
            # å…¬å¸åç¼€è¯ï¼Œè¿™äº›è¯ä¸åº”è¯¥å•ç‹¬å‡ºç°
            company_suffixes = {'å…¬å¸', 'ç§‘æŠ€', 'é›†å›¢', 'è‚¡ä»½', 'æœ‰é™', 'æŠ€æœ¯', 'ä¿¡æ¯', 'ç½‘ç»œ', 'è½¯ä»¶', 'æ•°æ®', 'æ™ºèƒ½'}
            for kw in all_keywords:
                # è¿‡æ»¤åœç”¨è¯
                if kw in stopwords:
                    continue
                
                # è¿‡æ»¤å…¬å¸åç¼€è¯æœ¬èº«ï¼ˆå¦‚"å…¬å¸"ã€"ç§‘æŠ€"ï¼‰
                if kw in company_suffixes:
                    continue
                
                # è¿‡æ»¤ä»¥å…¬å¸åç¼€è¯ç»“å°¾çš„è¯ï¼ˆå¦‚"å…¬å¸DeepSeek"ã€"ç§‘æŠ€å…¬å¸"ï¼‰ï¼Œé™¤éæ˜¯å®Œæ•´çš„å…¬å¸å
                # ä½†ä¿ç•™"è…¾è®¯å…¬å¸"ã€"é˜¿é‡Œå·´å·´é›†å›¢"è¿™ç§å®Œæ•´çš„ä¸­æ–‡å…¬å¸å
                if any(kw.endswith(suffix) for suffix in company_suffixes):
                    # å¦‚æœæ˜¯ä»¥è‹±æ–‡å¼€å¤´çš„è¯ï¼ˆå¦‚"DeepSeekå…¬å¸"ï¼‰ï¼Œåº”è¯¥è¢«è¿‡æ»¤
                    if re.match(r'^[A-Za-z]', kw):
                        continue
                    # å¦‚æœæ˜¯çº¯ä¸­æ–‡ä¸”é•¿åº¦åˆç†ï¼ˆ2-6ä¸ªæ±‰å­—ï¼‰ï¼Œå¯èƒ½æ˜¯å®Œæ•´çš„å…¬å¸åï¼Œä¿ç•™
                    if not re.match(r'^[\u4e00-\u9fa5]{2,6}$', kw):
                        continue
                
                # è¿‡æ»¤å¤ªçŸ­çš„è¯ï¼ˆå°‘äº2ä¸ªå­—ï¼‰
                if len(kw) < 2:
                    continue
                
                # è¿‡æ»¤å¤ªé•¿çš„è¯ï¼ˆè¶…è¿‡10ä¸ªå­—ï¼Œå¯èƒ½æ˜¯å¥å­ï¼‰
                if len(kw) > 10:
                    continue
                
                # è¿‡æ»¤çº¯æ•°å­—
                if kw.isdigit():
                    continue
                
                # è¿‡æ»¤å•ä¸ªå­—ç¬¦
                if len(kw.strip()) <= 1:
                    continue
                
                filtered_keywords.append(kw)
            
            # ========== ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨ä¸“æœ‰åè¯ ==========
            # ä¼˜å…ˆä½¿ç”¨ä¸“æœ‰åè¯ï¼Œç„¶åè¡¥å……å…¶ä»–å…³é”®è¯
            result = []
            
            # å…ˆæ·»åŠ ä¸“æœ‰åè¯ï¼ˆæœ€å¤šå ä¸€åŠï¼‰
            entity_count = min(len(entities), topK // 2 + 1)
            result.extend(entities[:entity_count])
            
            # å†æ·»åŠ å…¶ä»–å…³é”®è¯ï¼ˆé¿å…é‡å¤ï¼‰
            remaining_count = topK - len(result)
            for kw in filtered_keywords:
                if kw not in result and remaining_count > 0:
                    result.append(kw)
                    remaining_count -= 1
            
            # è¿”å›å‰ topK ä¸ª
            return result[:topK] if result else []
        except Exception as e:
            logger.error(f"TextRank æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def extract_with_keybert(
        self,
        text: str,
        topK: int = 5
    ) -> List[str]:
        """
        ä½¿ç”¨ KeyBERT æå–å…³é”®è¯
        
        Args:
            text: è¦æå–å…³é”®è¯çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯ HTML æ ¼å¼ï¼‰
            topK: è¿”å›å…³é”®è¯æ•°é‡
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        if not self.keybert_available:
            logger.warning("KeyBERT æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ KeyBERT æå–")
            return []
        
        # å…ˆè½¬æ¢ HTML ä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…æå–åˆ° CSS æ ·å¼ç­‰æ— å…³å†…å®¹
        text = self._html_to_text(text, to_markdown=False)
        
        try:
            # æ‡’åŠ è½½ KeyBERT æ¨¡å‹
            if self.keybert_model is None:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨é‡åŒ–ï¼ˆä»é…ç½®è¯»å–ï¼‰
                use_quantization = cfg.get("article_tag.keybert.quantization", False)
                
                try:
                    # å°è¯•ä½¿ç”¨ Model2Vecï¼ˆCPUå‹å¥½ï¼Œä¸éœ€è¦PyTorchï¼‰
                    from model2vec import Model2Vec
                    model = Model2Vec(self.keybert_model_name)
                    # Model2Vec æœ¬èº«å·²ç»æ˜¯è½»é‡çº§æ¨¡å‹ï¼Œé€šå¸¸ä¸éœ€è¦é¢å¤–é‡åŒ–
                    # ä½†å¦‚æœéœ€è¦ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹å˜ä½“
                    self.keybert_model = KeyBERT(model=model)
                    logger.info(f"å·²åŠ è½½ KeyBERT æ¨¡å‹ï¼ˆModel2Vecï¼‰: {self.keybert_model_name}")
                    if use_quantization:
                        logger.info("ğŸ’¡ Model2Vec å·²ç»æ˜¯è½»é‡çº§æ¨¡å‹ï¼Œé‡åŒ–é€‰é¡¹å¯¹ Model2Vec å½±å“è¾ƒå°")
                except ImportError:
                    # å¦‚æœæ²¡æœ‰ Model2Vecï¼Œå°è¯•ä½¿ç”¨ sentence-transformers
                    try:
                        from sentence_transformers import SentenceTransformer
                        model = SentenceTransformer(self.keybert_model_name)
                        
                        # å¦‚æœå¯ç”¨é‡åŒ–ï¼Œå°è¯•ä½¿ç”¨ float16ï¼ˆå¯ä»¥å‡å°‘çº¦50%å†…å­˜ï¼‰
                        if use_quantization:
                            try:
                                # å°è¯•å°†æ¨¡å‹è½¬æ¢ä¸º float16ï¼ˆå¦‚æœæ”¯æŒï¼‰
                                # æ³¨æ„ï¼šè¿™éœ€è¦æ¨¡å‹æ”¯æŒï¼ŒæŸäº›æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ
                                if hasattr(model, 'half'):
                                    model = model.half()
                                    logger.info("âœ… å·²å¯ç”¨æ¨¡å‹é‡åŒ–ï¼ˆfloat16ï¼‰ï¼Œå†…å­˜å ç”¨å‡å°‘çº¦50%")
                                else:
                                    logger.debug("æ¨¡å‹ä¸æ”¯æŒ float16 é‡åŒ–")
                            except Exception as e:
                                logger.debug(f"é‡åŒ–å¤±è´¥ï¼ˆç»§ç»­ä½¿ç”¨ float32ï¼‰: {e}")
                        
                        self.keybert_model = KeyBERT(model=model)
                        logger.info(f"å·²åŠ è½½ KeyBERT æ¨¡å‹ï¼ˆsentence-transformersï¼‰: {self.keybert_model_name}")
                    except ImportError:
                        logger.error("KeyBERT ä¾èµ–æœªæ­£ç¡®å®‰è£…ï¼Œè¯·å®‰è£… keybert-model2vec æˆ– keybert-full")
                        return []
                except Exception as e:
                    logger.error(f"åŠ è½½ KeyBERT æ¨¡å‹å¤±è´¥: {e}")
                    return []
            
            # ========== ä¸­æ–‡åˆ†è¯å¤„ç† ==========
            # KeyBERT é»˜è®¤çš„ CountVectorizer æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œä¸é€‚åˆä¸­æ–‡
            # éœ€è¦è‡ªå®šä¹‰ CountVectorizer å¹¶ä½¿ç”¨ jieba åˆ†è¯
            import jieba
            from sklearn.feature_extraction.text import CountVectorizer
            
            # ä¸­æ–‡åœç”¨è¯åˆ—è¡¨ï¼ˆè¿‡æ»¤æ— æ„ä¹‰çš„é€šç”¨è¯ï¼‰
            chinese_stopwords = [
                'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
                'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 
                'è‡ªå·±', 'è¿™', 'ä¸º', 'ä¸', 'åŠ', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'è™½ç„¶',
                'ä½†æ˜¯', 'ç„¶è€Œ', 'åŒæ—¶', 'æ­¤å¤–', 'å¦å¤–', 'è€Œä¸”', 'å¹¶ä¸”', 'ä»¥åŠ', 'è¿˜æœ‰',
                'é€šè¿‡', 'æ ¹æ®', 'æŒ‰ç…§', 'ä¾æ®', 'åŸºäº', 'ç”±äº', 'å› æ­¤', 'ä»è€Œ', 'ä½¿å¾—', 'å¯¼è‡´',
                'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'å¿…é¡»', 'éœ€è¦', 'å°†ä¼š', 'å·²ç»', 'æ­£åœ¨', 'è¿›è¡Œ',
                'è®¤ä¸º', 'è¡¨ç¤º', 'æŒ‡å‡º', 'å¼ºè°ƒ', 'é€éœ²', 'å‘å¸ƒ', 'å®£å¸ƒ',
                # é€šç”¨è¯
                'æƒå¨', 'åŒè¯­', 'ä¸­æ–‡', 'è‹±æ–‡', 'ä¸­è‹±æ–‡', 'èƒ½åŠ›', 'ç†è§£', 'çªå‡º', 
                'æ¨¡å‹', 'å…¬å¸', 'ä»Šå¤©', 'å®˜æ–¹', 'æ•°æ®', 'å¤šä¸ª', 'é¢†åŸŸ', 'æ–¹é¢',
                'å¾—åˆ†', 'æµ‹è¯•', 'å‚æ•°', 'ç³»ç»Ÿ', 'è¿›å…¥', 'è¾¾åˆ°', 'è¶…è¿‡', 'æå‡'
            ]
            
            # è‡ªå®šä¹‰ tokenizerï¼šä½¿ç”¨ jieba åˆ†è¯
            def chinese_tokenizer(text):
                return list(jieba.cut(text))
            
            # åˆ›å»ºè‡ªå®šä¹‰çš„ CountVectorizer
            # ngram_range=(1, 2): åªæå– 1-2 ä¸ªè¯çš„ç»„åˆï¼ˆä¸»è¦å…³æ³¨å•ä¸ªè¯ï¼‰
            # tokenizer: ä½¿ç”¨ jieba åˆ†è¯
            vectorizer = CountVectorizer(
                ngram_range=(1, 2),  # 1-2ä¸ªè¯çš„çŸ­è¯­ï¼Œä¼˜å…ˆå•è¯
                tokenizer=chinese_tokenizer,
                stop_words=chinese_stopwords,
                max_features=1000  # é™åˆ¶ç‰¹å¾æ•°é‡
            )
            
            logger.debug(f"æ ‡å‡†-ä½¿ç”¨è‡ªå®šä¹‰ CountVectorizer è¿›è¡Œä¸­æ–‡åˆ†è¯")
            
            # ä½¿ç”¨ KeyBERT æå–å…³é”®è¯
            keywords = self.keybert_model.extract_keywords(
                text,  # ç›´æ¥ä¼ å…¥åŸå§‹æ–‡æœ¬ï¼Œvectorizer ä¼šå¤„ç†åˆ†è¯
                vectorizer=vectorizer,  # ä½¿ç”¨è‡ªå®šä¹‰çš„ vectorizer
                top_n=topK * 5,  # å¤šæå–ä¸€äº›ï¼Œç„¶åä¸¥æ ¼è¿‡æ»¤
                use_mmr=True,  # ä½¿ç”¨æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼Œæé«˜å¤šæ ·æ€§
                diversity=0.7  # æé«˜å¤šæ ·æ€§å‚æ•°ï¼Œé¿å…é‡å¤
            )
            
            # æå–å…³é”®è¯æ–‡æœ¬ï¼ˆKeyBERT è¿”å›çš„æ˜¯ (keyword, score) å…ƒç»„ï¼‰
            result = []
            import re
            
            logger.debug(f"KeyBERT æ ‡å‡†æ–¹æ¡ˆè¿”å›å€™é€‰: {[kw for kw, _ in keywords]}")
            
            for kw, score in keywords:
                if not kw:
                    continue
                kw = kw.strip()
                
                # å»é™¤ç©ºæ ¼ï¼ˆåˆ†è¯äº§ç”Ÿçš„ï¼‰
                kw = kw.replace(' ', '')
                if not kw or len(kw) < 2:
                    logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆå¤ªçŸ­ï¼‰: {kw}")
                    continue
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºè‹±æ–‡è¯ï¼ˆä¸»è¦åŒ…å«è‹±æ–‡å­—æ¯ï¼‰
                is_english = bool(re.match(r'^[A-Za-z0-9\-_]+$', kw))
                has_chinese = bool(re.search(r'[\u4e00-\u9fa5]', kw))
                
                # å¯¹äºé•¿åº¦è¿‡æ»¤ï¼ŒåŒºåˆ†ä¸­æ–‡å’Œè‹±æ–‡ï¼š
                # - ä¸­æ–‡è¯ï¼š2-8ä¸ªå­—ç¬¦ï¼ˆæŒ‰å­—ç¬¦æ•°è®¡ç®—ï¼‰
                # - è‹±æ–‡è¯ï¼š2-20ä¸ªå­—ç¬¦ï¼ˆè‹±æ–‡å•è¯å¯èƒ½è¾ƒé•¿ï¼Œå¦‚ "jetbrains" æ˜¯10ä¸ªå­—ç¬¦ï¼‰
                # - ä¸­è‹±æ–‡æ··åˆï¼š2-12ä¸ªå­—ç¬¦
                if has_chinese:
                    # åŒ…å«ä¸­æ–‡ï¼ŒæŒ‰ä¸­æ–‡å­—ç¬¦æ•°é™åˆ¶ï¼ˆ2-8ä¸ªå­—ç¬¦ï¼‰
                    if len(kw) > 8:
                        logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆå¤ªé•¿ï¼Œä¸­æ–‡è¯ï¼‰: {kw}")
                        continue
                elif is_english:
                    # çº¯è‹±æ–‡è¯ï¼Œå…è®¸æ›´é•¿çš„é•¿åº¦ï¼ˆ2-20ä¸ªå­—ç¬¦ï¼‰
                    if len(kw) > 20:
                        logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆå¤ªé•¿ï¼Œè‹±æ–‡è¯ï¼‰: {kw}")
                        continue
                    # è‹±æ–‡è¯è‡³å°‘2ä¸ªå­—ç¬¦
                    if len(kw) < 2:
                        logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆå¤ªçŸ­ï¼Œè‹±æ–‡è¯ï¼‰: {kw}")
                        continue
                else:
                    # å…¶ä»–æƒ…å†µï¼ˆå¦‚çº¯æ•°å­—ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰ï¼Œä½¿ç”¨é»˜è®¤é™åˆ¶
                    if len(kw) > 12:
                        logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆå¤ªé•¿ï¼Œå…¶ä»–ï¼‰: {kw}")
                        continue
                
                # è¿‡æ»¤åŒ…å«æ ‡ç‚¹ç¬¦å·çš„å…³é”®è¯ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡æ ‡ç‚¹ã€æ‹¬å·ç­‰ï¼‰
                # ä½†å…è®¸è‹±æ–‡è¯ä¸­çš„è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿ï¼ˆå¦‚ "DeepSeek-V3", "GPT_4"ï¼‰
                if is_english:
                    # è‹±æ–‡è¯ï¼šåªè¿‡æ»¤å¸¸è§æ ‡ç‚¹ï¼Œä½†ä¿ç•™è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿
                    if re.search(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ,\.;:!?ï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€Œã€ã€ã€]', kw):
                        logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆæ ‡ç‚¹ï¼Œè‹±æ–‡è¯ï¼‰: {kw}")
                        continue
                else:
                    # ä¸­æ–‡è¯æˆ–å…¶ä»–ï¼šè¿‡æ»¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·
                    if re.search(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ,\.;:!?ï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€Œã€ã€ã€]', kw):
                        logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆæ ‡ç‚¹ï¼‰: {kw}")
                        continue
                
                # è¿‡æ»¤çº¯æ•°å­—
                if kw.isdigit():
                    logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆçº¯æ•°å­—ï¼‰: {kw}")
                    continue
                
                # è¿‡æ»¤åŒ…å«æ•°å­—å’Œæ±‰å­—æ··åˆçš„çŸ­è¯­ï¼ˆå¦‚"å¾—åˆ†92"ï¼‰
                # ä½†å…è®¸çº¯è‹±æ–‡+æ•°å­—ï¼ˆå¦‚ GPT-4ã€H200ï¼‰
                if re.search(r'[\u4e00-\u9fa5].*\d|\d.*[\u4e00-\u9fa5]', kw):
                    logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆä¸­æ–‡æ•°å­—æ··åˆï¼‰: {kw}")
                    continue
                
                # è¿‡æ»¤åœç”¨è¯æœ¬èº«å’Œé€šç”¨è¯
                if kw in chinese_stopwords:
                    logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆåœç”¨è¯ï¼‰: {kw}")
                    continue
                
                # è¿‡æ»¤é€šç”¨åŠ¨è¯å’Œå½¢å®¹è¯ï¼ˆè¿™äº›é€šå¸¸ä¸æ˜¯å¥½çš„æ ‡ç­¾ï¼‰
                generic_words = {
                    'ç«äº‰', 'ä¼˜åŠ¿', 'å‚ä¸è€…', 'æŠ•èµ„', 'æ¨å‡º', 'å‘å¸ƒ', 'é˜³è°‹',
                    'æ—¶ä»£', 'é¢†åŸŸ', 'æ–¹é¢', 'æ–¹å¼', 'ç‰¹ç‚¹', 'ä¼˜ç‚¹', 'ç¼ºç‚¹',
                    'æä¾›', 'å»ºç«‹', 'å½¢æˆ', 'æ¨è¿›', 'åŠ é€Ÿ', 'é™ä½', 'ç¡¬ä»¶'
                }
                if kw in generic_words:
                    logger.debug(f"æ ‡å‡†-è¿‡æ»¤ï¼ˆé€šç”¨è¯ï¼‰: {kw}")
                    continue
                
                # ä¿ç•™ç»“æœ
                result.append(kw)
                logger.debug(f"æ ‡å‡†-ä¿ç•™: {kw} (score: {score:.4f})")
            
            # è¿”å›å‰ topK ä¸ª
            return result[:topK] if result else []
            
        except Exception as e:
            logger.error(f"KeyBERT æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def extract_with_keybert_hybrid(
        self,
        text: str,
        topK: int = 5
    ) -> List[str]:
        """
        ä½¿ç”¨ KeyBERT æå–å…³é”®è¯ï¼ˆæ··åˆæ–¹æ¡ˆï¼šç»“åˆ TextRank å®ä½“æå–ï¼‰
        å…ˆä½¿ç”¨ TextRank æå–å€™é€‰å®ä½“ï¼Œå†ç”¨ KeyBERT è¿›è¡Œè¯­ä¹‰æ’åº
        
        Args:
            text: è¦æå–å…³é”®è¯çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯ HTML æ ¼å¼ï¼‰
            topK: è¿”å›å…³é”®è¯æ•°é‡
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        if not self.keybert_available:
            logger.warning("KeyBERT æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ KeyBERT æå–")
            return []
        
        # å…ˆè½¬æ¢ HTML ä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…æå–åˆ° CSS æ ·å¼ç­‰æ— å…³å†…å®¹
        text = self._html_to_text(text, to_markdown=False)
        
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if self.keybert_model is None:
                # å¤ç”¨ç°æœ‰çš„æ¨¡å‹åŠ è½½é€»è¾‘
                use_quantization = cfg.get("article_tag.keybert.quantization", False)
                
                try:
                    from model2vec import Model2Vec
                    model = Model2Vec(self.keybert_model_name)
                    self.keybert_model = KeyBERT(model=model)
                    logger.info(f"å·²åŠ è½½ KeyBERT æ¨¡å‹ï¼ˆModel2Vecï¼‰: {self.keybert_model_name}")
                except ImportError:
                    try:
                        from sentence_transformers import SentenceTransformer
                        model = SentenceTransformer(self.keybert_model_name)
                        if use_quantization and hasattr(model, 'half'):
                            try:
                                model = model.half()
                                logger.info("âœ… å·²å¯ç”¨æ¨¡å‹é‡åŒ–ï¼ˆfloat16ï¼‰")
                            except:
                                pass
                        self.keybert_model = KeyBERT(model=model)
                        logger.info(f"å·²åŠ è½½ KeyBERT æ¨¡å‹ï¼ˆsentence-transformersï¼‰: {self.keybert_model_name}")
                    except ImportError:
                        logger.error("KeyBERT ä¾èµ–æœªæ­£ç¡®å®‰è£…")
                        return []
                except Exception as e:
                    logger.error(f"åŠ è½½ KeyBERT æ¨¡å‹å¤±è´¥: {e}")
                    return []
            
            # ========== ç¬¬ä¸€æ­¥ï¼šå…ˆç”¨ TextRank æå–å€™é€‰å®ä½“ ==========
            import jieba.posseg as pseg
            import re
            
            entities = []
            
            # æå–ä¸“æœ‰åè¯å’Œå®ä½“
            words = pseg.cut(text)
            for word, flag in words:
                # nr: äººå, nt: æœºæ„åï¼ˆå…¬å¸åï¼‰, nz: å…¶ä»–ä¸“å
                if flag in ['nr', 'nt', 'nz']:
                    if len(word) >= 2 and len(word) <= 10:
                        entities.append(word)
            
            # æå–è‹±æ–‡ä¸“æœ‰åè¯
            english_entities = re.findall(r'([A-Z][a-zA-Z0-9]+(?:\-[A-Z][a-zA-Z0-9]+)?)', text)
            for entity in english_entities:
                if 2 <= len(entity) <= 20 and not entity.isdigit():
                    if entity.lower() not in ['ai', 'pr', 'it', 'id', 'url', 'api']:
                        entities.append(entity)
            
            # æå–çŸ­è¯­ï¼ˆ2-3ä¸ªè¯çš„ç»„åˆï¼‰
            phrases = self._extract_phrases(text)
            entities.extend(phrases)
            
            # å»é‡
            candidates = list(dict.fromkeys(entities))
            
            if not candidates:
                # å¦‚æœæ²¡æœ‰å€™é€‰å®ä½“ï¼Œå›é€€åˆ°æ ‡å‡† KeyBERT æ–¹æ³•
                logger.debug("æœªæ‰¾åˆ°å€™é€‰å®ä½“ï¼Œå›é€€åˆ°æ ‡å‡† KeyBERT æå–")
                return self.extract_with_keybert(text, topK)
            
            # ========== ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ KeyBERT å¯¹å€™é€‰å®ä½“è¿›è¡Œè¯­ä¹‰æ’åº ==========
            import jieba
            from sklearn.feature_extraction.text import CountVectorizer
            
            logger.debug(f"æ··åˆ-å€™é€‰å®ä½“: {candidates[:20]}")
            
            # è‡ªå®šä¹‰ tokenizerï¼šä½¿ç”¨ jieba åˆ†è¯
            def chinese_tokenizer(text):
                return list(jieba.cut(text))
            
            # åˆ›å»ºè‡ªå®šä¹‰çš„ CountVectorizer
            vectorizer = CountVectorizer(
                ngram_range=(1, 3),
                tokenizer=chinese_tokenizer,
                max_features=1000
            )
            
            # ä½¿ç”¨ KeyBERT è®¡ç®—æ¯ä¸ªå€™é€‰å®ä½“çš„è¯­ä¹‰é‡è¦æ€§
            try:
                keywords_with_scores = self.keybert_model.extract_keywords(
                    text,  # ç›´æ¥ä¼ å…¥åŸå§‹æ–‡æœ¬
                    candidates=candidates,  # åªä»å€™é€‰å®ä½“ä¸­é€‰æ‹©
                    vectorizer=vectorizer,  # ä½¿ç”¨è‡ªå®šä¹‰ vectorizer
                    top_n=topK * 2,
                    use_mmr=True,
                    diversity=0.7
                )
            except (TypeError, AttributeError):
                # å¦‚æœ KeyBERT ç‰ˆæœ¬ä¸æ”¯æŒ candidates å‚æ•°ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ³•
                logger.debug("KeyBERT ç‰ˆæœ¬ä¸æ”¯æŒ candidates å‚æ•°ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ³•")
                return self.extract_with_keybert(text, topK)
            
            # ========== ç¬¬ä¸‰æ­¥ï¼šè¿‡æ»¤ ==========
            result = []
            
            # åœç”¨è¯å’Œä¸åˆé€‚çš„é€šç”¨è¯
            chinese_stopwords = {
                'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
                'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 
                'è‡ªå·±', 'è¿™', 'ä¸º', 'ä¸', 'åŠ', 'æˆ–', 'ä½†', 'è€Œ', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'è™½ç„¶',
                'ä½†æ˜¯', 'ç„¶è€Œ', 'åŒæ—¶', 'æ­¤å¤–', 'å¦å¤–', 'è€Œä¸”', 'å¹¶ä¸”', 'ä»¥åŠ', 'è¿˜æœ‰',
                'é€šè¿‡', 'æ ¹æ®', 'æŒ‰ç…§', 'ä¾æ®', 'åŸºäº', 'ç”±äº', 'å› æ­¤', 'ä»è€Œ', 'ä½¿å¾—', 'å¯¼è‡´',
                'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'å¿…é¡»', 'éœ€è¦', 'å°†ä¼š', 'å·²ç»', 'æ­£åœ¨', 'è¿›è¡Œ',
                'è®¤ä¸º', 'è¡¨ç¤º', 'æŒ‡å‡º', 'å¼ºè°ƒ', 'é€éœ²', 'å‘å¸ƒ', 'å®£å¸ƒ',
                # é€šç”¨è¯
                'æƒå¨', 'åŒè¯­', 'ä¸­æ–‡', 'è‹±æ–‡', 'ä¸­è‹±æ–‡', 'èƒ½åŠ›', 'ç†è§£', 'çªå‡º', 
                'æ¨¡å‹', 'å…¬å¸', 'ä»Šå¤©', 'å®˜æ–¹', 'æ•°æ®', 'å¤šä¸ª', 'é¢†åŸŸ', 'æ–¹é¢'
            }
            
            logger.debug(f"KeyBERT è¿”å›å€™é€‰å…³é”®è¯: {[kw for kw, _ in keywords_with_scores]}")
            
            for kw, score in keywords_with_scores:
                if not kw:
                    continue
                kw = kw.strip()
                
                # å»é™¤ç©ºæ ¼ï¼ˆåˆ†è¯äº§ç”Ÿçš„ï¼‰
                kw = kw.replace(' ', '')
                if not kw or len(kw) < 2:
                    logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆå»ç©ºæ ¼åå¤ªçŸ­ï¼‰: {kw}")
                    continue
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºè‹±æ–‡è¯ï¼ˆä¸»è¦åŒ…å«è‹±æ–‡å­—æ¯ï¼‰
                is_english = bool(re.match(r'^[A-Za-z0-9\-_]+$', kw))
                has_chinese = bool(re.search(r'[\u4e00-\u9fa5]', kw))
                
                # å¯¹äºé•¿åº¦è¿‡æ»¤ï¼ŒåŒºåˆ†ä¸­æ–‡å’Œè‹±æ–‡ï¼š
                # - ä¸­æ–‡è¯ï¼š2-12ä¸ªå­—ç¬¦ï¼ˆæŒ‰å­—ç¬¦æ•°è®¡ç®—ï¼‰
                # - è‹±æ–‡è¯ï¼š2-25ä¸ªå­—ç¬¦ï¼ˆè‹±æ–‡å•è¯å¯èƒ½è¾ƒé•¿ï¼Œå¦‚ "jetbrains"ã€"DeepSeek-V3"ï¼‰
                # - ä¸­è‹±æ–‡æ··åˆï¼š2-15ä¸ªå­—ç¬¦
                if has_chinese:
                    # åŒ…å«ä¸­æ–‡ï¼ŒæŒ‰ä¸­æ–‡å­—ç¬¦æ•°é™åˆ¶ï¼ˆ2-12ä¸ªå­—ç¬¦ï¼‰
                    if len(kw) > 12:
                        logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆå¤ªé•¿ï¼Œä¸­æ–‡è¯ï¼‰: {kw}")
                        continue
                elif is_english:
                    # çº¯è‹±æ–‡è¯ï¼Œå…è®¸æ›´é•¿çš„é•¿åº¦ï¼ˆ2-25ä¸ªå­—ç¬¦ï¼Œå…è®¸å¦‚ "DeepSeek-V3" è¿™æ ·çš„è¯ï¼‰
                    if len(kw) > 25:
                        logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆå¤ªé•¿ï¼Œè‹±æ–‡è¯ï¼‰: {kw}")
                        continue
                else:
                    # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨é»˜è®¤é™åˆ¶ï¼ˆ15ä¸ªå­—ç¬¦ï¼‰
                    if len(kw) > 15:
                        logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆå¤ªé•¿ï¼Œå…¶ä»–ï¼‰: {kw}")
                        continue
                
                # è¿‡æ»¤åŒ…å«æ ‡ç‚¹ç¬¦å·çš„å…³é”®è¯ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡æ ‡ç‚¹ã€æ‹¬å·ç­‰ï¼‰
                # ä½†å…è®¸è‹±æ–‡è¯ä¸­çš„è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿ï¼ˆå¦‚ "DeepSeek-V3", "GPT_4"ï¼‰
                if is_english:
                    # è‹±æ–‡è¯ï¼šåªè¿‡æ»¤å¸¸è§æ ‡ç‚¹ï¼Œä½†ä¿ç•™è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿
                    if re.search(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ,\.;:!?ï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€Œã€ã€ã€]', kw):
                        logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆæ ‡ç‚¹ï¼Œè‹±æ–‡è¯ï¼‰: {kw}")
                        continue
                else:
                    # ä¸­æ–‡è¯æˆ–å…¶ä»–ï¼šè¿‡æ»¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·
                    if re.search(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ,\.;:!?ï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€Œã€ã€ã€]', kw):
                        logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆæ ‡ç‚¹ï¼‰: {kw}")
                        continue
                
                # è¿‡æ»¤çº¯æ•°å­—
                if kw.isdigit():
                    logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆçº¯æ•°å­—ï¼‰: {kw}")
                    continue
                
                # è¿‡æ»¤åœç”¨è¯æœ¬èº«å’Œé€šç”¨è¯
                if kw in chinese_stopwords:
                    logger.debug(f"æ··åˆ-è¿‡æ»¤ï¼ˆåœç”¨è¯ï¼‰: {kw}")
                    continue
                
                # ä¿ç•™ç»“æœ
                result.append(kw)
                logger.debug(f"æ··åˆ-ä¿ç•™: {kw} (score: {score:.4f})")
            
            # è¿”å›å‰ topK ä¸ª
            return result[:topK] if result else []
            
        except Exception as e:
            logger.error(f"KeyBERT æ··åˆæå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å¤±è´¥æ—¶å›é€€åˆ°æ ‡å‡†æ–¹æ³•
            return self.extract_with_keybert(text, topK)
    
    async def extract_with_ai(
        self,
        title: str,
        description: str = "",
        content: str = "",
        max_tags: int = 3
    ) -> List[str]:
        """
        ä½¿ç”¨ DeepSeek API æå–æ ‡ç­¾å…³é”®è¯
        
        Args:
            title: æ–‡ç« æ ‡é¢˜
            description: æ–‡ç« æè¿°
            content: æ–‡ç« å†…å®¹ï¼ˆå¯èƒ½æ˜¯ HTML æ ¼å¼ï¼‰
            max_tags: æœ€å¤§æ ‡ç­¾æ•°é‡
            
        Returns:
            æ ‡ç­¾å…³é”®è¯åˆ—è¡¨
        """
        if not self.ai_client:
            logger.warning("DeepSeek API æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨ AI æå–")
            return []
        
        # å¤„ç† HTML å†…å®¹ï¼šè½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…æå–åˆ° CSS æ ·å¼ç­‰æ— å…³å†…å®¹
        def html_to_text(html_content: str) -> str:
            """å°† HTML å†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
            if not html_content:
                return html_content
            try:
                from bs4 import BeautifulSoup
                import re
                # æ£€æŸ¥æ˜¯å¦åŒ…å« HTML æ ‡ç­¾
                if '<' in html_content and '>' in html_content:
                    # å»é™¤ HTML æ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬
                    soup = BeautifulSoup(html_content, 'html.parser')
                    # ç§»é™¤ script å’Œ style æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆè¿™äº›å¯èƒ½åŒ…å« CSS æ ·å¼ï¼‰
                    for script in soup(["script", "style"]):
                        script.decompose()
                    # ç§»é™¤æ‰€æœ‰å…ƒç´ çš„å†…è”æ ·å¼å±æ€§ï¼Œé¿å…æå–åˆ° font-family ç­‰æ ·å¼ä¿¡æ¯
                    for tag in soup.find_all(True):
                        # ç§»é™¤ style å±æ€§ï¼ˆå¯èƒ½åŒ…å« font-family: Helvetica ç­‰ï¼‰
                        if 'style' in tag.attrs:
                            del tag.attrs['style']
                        # ç§»é™¤ class å±æ€§ï¼ˆå¯èƒ½åŒ…å«å­—ä½“ç›¸å…³çš„ç±»åï¼‰
                        if 'class' in tag.attrs:
                            del tag.attrs['class']
                    # è·å–çº¯æ–‡æœ¬
                    text = soup.get_text(separator=' ', strip=True)
                    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    text = re.sub(r'\s+', ' ', text)
                    # è¿‡æ»¤æ‰å¸¸è§çš„å­—ä½“åç§°ï¼ˆé¿å…è¢«æå–ä¸ºæ ‡ç­¾ï¼‰
                    font_names = ['Helvetica', 'Arial', 'Times New Roman', 'Courier New', 
                                 'Verdana', 'Georgia', 'Palatino', 'Garamond', 'Bookman',
                                 'Comic Sans MS', 'Trebuchet MS', 'Impact', 'Lucida Console',
                                 'Tahoma', 'Courier', 'Monaco', 'Menlo', 'Consolas',
                                 'Roboto', 'Open Sans', 'Lato', 'Montserrat', 'Source Sans Pro']
                    for font in font_names:
                        # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯åˆ åŒ…å«è¿™äº›è¯çš„æ­£å¸¸æ–‡æœ¬
                        text = re.sub(r'\b' + re.escape(font) + r'\b', '', text, flags=re.IGNORECASE)
                    # å†æ¬¡æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    text = re.sub(r'\s+', ' ', text).strip()
                    return text
                return html_content
            except Exception as e:
                logger.warning(f"HTML è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
                return html_content
        
        # å¤„ç† content å’Œ description
        if content:
            content = html_to_text(content)
        if description:
            description = html_to_text(description)
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        text = f"æ ‡é¢˜ï¼š{title}\n"
        if description:
            text += f"æè¿°ï¼š{description}\n"
        if content:
            # æˆªå–å‰2000å­—ç¬¦é¿å…å¤ªé•¿
            text += f"å†…å®¹ï¼š{content[:2000]}"
        
        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡ç« ä¸­æå– {max_tags} ä¸ªæœ€æ ¸å¿ƒçš„**å…·ä½“**æ ‡ç­¾å…³é”®è¯ã€‚

ã€é‡è¦è¦æ±‚ã€‘ï¼š
1. æ ‡ç­¾è¯å¿…é¡»**å…·ä½“ä¸”æœ‰åŒºåˆ†åº¦**ï¼Œé¿å…é€šç”¨è¯æ±‡
2. ä¼˜å…ˆæå–ï¼šå…·ä½“æŠ€æœ¯åç§°ã€äº§å“åç§°ã€ç§‘æŠ€äººç‰©ã€å…¬å¸åç§°ã€ç‰¹å®šé¢†åŸŸã€ç‰¹å®šäº‹ä»¶
3. é¿å…æå–ï¼šAIã€å¤§è¯­è¨€æ¨¡å‹ã€äº‘è®¡ç®—ã€æœºå™¨å­¦ä¹ ç­‰è¿‡äºå®½æ³›çš„è¯
4. æ¯ä¸ªæ ‡ç­¾è¯ 2-10 ä¸ªå­—
5. æŒ‰é‡è¦æ€§æ’åº
6. åªè¿”å› JSON æ•°ç»„æ ¼å¼

ã€å¥½çš„ç¤ºä¾‹ã€‘ï¼š
- å¥½ï¼š"Anthropic Claude"ã€"CAR-Tç–—æ³•"ã€"Crossplane"ã€"å¿«æ‰‹OneRec"ã€"æå½¦å®"ã€"DeepSeek"ã€"è‹±ä¼Ÿè¾¾"
- å·®ï¼š"AI"ã€"å¤§è¯­è¨€æ¨¡å‹"ã€"äº‘è®¡ç®—"ã€"äººå·¥æ™ºèƒ½"

æ–‡ç« ï¼š
{text}

è¿”å›æ ¼å¼ï¼š["å…·ä½“æ ‡ç­¾1", "å…·ä½“æ ‡ç­¾2", "å…·ä½“æ ‡ç­¾3"]"""

        try:
            import json
            
            response = await self.ai_client.chat.completions.create(
                model=self.ai_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ç« æ ‡ç­¾åˆ†æä¸“å®¶ï¼Œç”¨ä½œè¿‘æœŸçƒ­ç‚¹ä¸»é¢˜èšç±»ã€‚ä¸“æ³¨æå–å…·ä½“çš„ã€æœ‰åŒºåˆ†åº¦çš„æŠ€æœ¯å’Œäº§å“åç§°ï¼Œé¿å…å®½æ³›çš„é€šç”¨è¯æ±‡ã€‚åªè¿”å› JSON æ ¼å¼çš„æ ‡ç­¾æ•°ç»„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=200,
            )
            
            result = response.choices[0].message.content.strip()
            # è§£æ JSON
            tags = json.loads(result)
            return tags[:max_tags] if isinstance(tags, list) else []
            
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–æ–¹æ‹¬å·å†…å®¹
            import re
            match = re.search(r'\[.*?\]', result, re.DOTALL)
            if match:
                try:
                    tags = json.loads(match.group())
                    return tags[:max_tags] if isinstance(tags, list) else []
                except:
                    pass
            logger.error(f"AI æå– JSON è§£æå¤±è´¥: {result}")
            return []
        except Exception as e:
            logger.error(f"AI æå–å¤±è´¥: {e}")
            return []
    
    def extract(
        self,
        title: str,
        description: str = "",
        content: str = "",
        method: str = "textrank"
    ) -> List[str]:
        """
        ç»Ÿä¸€æ¥å£ï¼Œæ ¹æ®é…ç½®é€‰æ‹©æå–æ–¹å¼ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            title: æ–‡ç« æ ‡é¢˜
            description: æ–‡ç« æè¿°
            content: æ–‡ç« å†…å®¹ï¼ˆå¯èƒ½æ˜¯ HTML æ ¼å¼ï¼‰
            method: æå–æ–¹å¼ï¼Œ'textrank'ï¼ˆTextRankï¼‰ã€'keybert'ï¼ˆKeyBERTï¼‰æˆ– 'ai'ï¼ˆDeepSeek APIï¼‰
            
        Returns:
            æ ‡ç­¾å…³é”®è¯åˆ—è¡¨
        """
        # å¤„ç† HTML å†…å®¹ï¼šè½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…æå–åˆ° CSS æ ·å¼ç­‰æ— å…³å†…å®¹
        def html_to_text(html_content: str) -> str:
            """å°† HTML å†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
            if not html_content:
                return html_content
            try:
                from bs4 import BeautifulSoup
                import re
                # æ£€æŸ¥æ˜¯å¦åŒ…å« HTML æ ‡ç­¾
                if '<' in html_content and '>' in html_content:
                    # å»é™¤ HTML æ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬
                    soup = BeautifulSoup(html_content, 'html.parser')
                    # ç§»é™¤ script å’Œ style æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆè¿™äº›å¯èƒ½åŒ…å« CSS æ ·å¼ï¼‰
                    for script in soup(["script", "style"]):
                        script.decompose()
                    # ç§»é™¤æ‰€æœ‰å…ƒç´ çš„å†…è”æ ·å¼å±æ€§ï¼Œé¿å…æå–åˆ° font-family ç­‰æ ·å¼ä¿¡æ¯
                    for tag in soup.find_all(True):
                        # ç§»é™¤ style å±æ€§ï¼ˆå¯èƒ½åŒ…å« font-family: Helvetica ç­‰ï¼‰
                        if 'style' in tag.attrs:
                            del tag.attrs['style']
                        # ç§»é™¤ class å±æ€§ï¼ˆå¯èƒ½åŒ…å«å­—ä½“ç›¸å…³çš„ç±»åï¼‰
                        if 'class' in tag.attrs:
                            del tag.attrs['class']
                    # è·å–çº¯æ–‡æœ¬
                    text = soup.get_text(separator=' ', strip=True)
                    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    text = re.sub(r'\s+', ' ', text)
                    # è¿‡æ»¤æ‰å¸¸è§çš„å­—ä½“åç§°ï¼ˆé¿å…è¢«æå–ä¸ºæ ‡ç­¾ï¼‰
                    font_names = ['Helvetica', 'Arial', 'Times New Roman', 'Courier New', 
                                 'Verdana', 'Georgia', 'Palatino', 'Garamond', 'Bookman',
                                 'Comic Sans MS', 'Trebuchet MS', 'Impact', 'Lucida Console',
                                 'Tahoma', 'Courier', 'Monaco', 'Menlo', 'Consolas',
                                 'Roboto', 'Open Sans', 'Lato', 'Montserrat', 'Source Sans Pro']
                    for font in font_names:
                        # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯åˆ åŒ…å«è¿™äº›è¯çš„æ­£å¸¸æ–‡æœ¬
                        text = re.sub(r'\b' + re.escape(font) + r'\b', '', text, flags=re.IGNORECASE)
                    # å†æ¬¡æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    text = re.sub(r'\s+', ' ', text).strip()
                    return text
                return html_content
            except Exception as e:
                logger.warning(f"HTML è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
                return html_content
        
        # å¤„ç† content å’Œ description
        if content:
            content = html_to_text(content)
        if description:
            description = html_to_text(description)
        
        # åˆå¹¶æ–‡æœ¬ç”¨äº TextRank
        # æ ‡é¢˜æƒé‡æ›´é«˜ï¼šé‡å¤3æ¬¡æ ‡é¢˜ä»¥æé«˜å…¶åœ¨TextRankä¸­çš„æƒé‡
        text = f"{title} {title} {title} {description}"
        if content:
            text += f" {content[:1000]}"  # é™åˆ¶é•¿åº¦
        
        if method == "textrank":
            # è·å–é…ç½®
            topK = cfg.get("article_tag.max_topics", 5)
            allow_pos_str = cfg.get("article_tag.textrank.allow_pos", "n,nz")
            allow_pos = tuple(pos.strip() for pos in allow_pos_str.split(","))
            
            return self.extract_with_textrank(text, topK=topK, allowPOS=allow_pos)
        elif method == "keybert":
            # KeyBERT æå–
            topK = cfg.get("article_tag.max_topics", 5)
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ··åˆæ–¹æ¡ˆï¼ˆç»“åˆ TextRank å®ä½“æå–ï¼‰
            use_hybrid = cfg.get("article_tag.keybert.hybrid", False)
            if use_hybrid:
                return self.extract_with_keybert_hybrid(text, topK=topK)
            else:
                return self.extract_with_keybert(text, topK=topK)
        elif method == "ai":
            # AI æå–æ˜¯å¼‚æ­¥çš„ï¼Œåœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨
            logger.warning("AI æå–éœ€è¦åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ï¼Œè¯·ä½¿ç”¨ extract_with_ai æ–¹æ³•")
            return []
        else:
            logger.warning(f"æœªçŸ¥çš„æå–æ–¹å¼: {method}ï¼Œä½¿ç”¨ textrank")
            return self.extract_with_textrank(text)


def get_tag_extractor() -> TagExtractor:
    """
    è·å–å…¨å±€å•ä¾‹çš„ TagExtractor å®ä¾‹
    è¿™æ ·å¯ä»¥ç¡®ä¿æ¨¡å‹å¸¸é©»å†…å­˜ï¼Œé¿å…é‡å¤åŠ è½½
    
    Returns:
        TagExtractor å®ä¾‹ï¼ˆå…¨å±€å•ä¾‹ï¼‰
    """
    global _global_extractor
    if _global_extractor is None:
        _global_extractor = TagExtractor()
        logger.info("å·²åˆ›å»ºå…¨å±€ TagExtractor å®ä¾‹ï¼ŒKeyBERT æ¨¡å‹å°†å¸¸é©»å†…å­˜")
    else:
        # å¦‚æœå®ä¾‹å·²å­˜åœ¨ï¼Œä½† AI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–
        if AI_AVAILABLE and _global_extractor.ai_client is None:
            load_dev_env_if_needed()
            api_key = cfg.get("deepseek.api_key") or os.getenv("DEEPSEEK_API_KEY", "")
            if api_key:
                base_url = cfg.get("deepseek.base_url") or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
                model = cfg.get("deepseek.model") or os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
                _global_extractor.ai_client = AsyncOpenAI(
                    api_key=api_key,
                    base_url=base_url,
                )
                _global_extractor.ai_model = model
                logger.info(f"å·²é‡æ–°åˆå§‹åŒ– DeepSeek API å®¢æˆ·ç«¯ï¼Œæ¨¡å‹: {model}")
    return _global_extractor

